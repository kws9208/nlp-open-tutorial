{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pZGpKElo4PK-","executionInfo":{"status":"ok","timestamp":1693540614025,"user_tz":-540,"elapsed":25633,"user":{"displayName":"Wooseok Kim (Wooseok)","userId":"10829587564796253131"}},"outputId":"e9b3b4c1-2dc5-4b58-c6e9-d2525969ac27"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kz49ImzT4QrU","executionInfo":{"status":"ok","timestamp":1693540636900,"user_tz":-540,"elapsed":14710,"user":{"displayName":"Wooseok Kim (Wooseok)","userId":"10829587564796253131"}},"outputId":"bf6b01dd-2cd2-4399-987d-9e3f0fb565be"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.1\n"]}]},{"cell_type":"code","execution_count":57,"metadata":{"id":"ACkVVaG14MB7","executionInfo":{"status":"ok","timestamp":1693542447613,"user_tz":-540,"elapsed":377,"user":{"displayName":"Wooseok Kim (Wooseok)","userId":"10829587564796253131"}}},"outputs":[],"source":["import json\n","import torch\n","from torch.utils.data import Dataset\n","\n","\n","class myDataset(Dataset):\n","  def __init__(self, data_path, tokenizer) -> None:\n","    super().__init__()\n","    self.json_data = []\n","    with open(data_path, 'r') as f:\n","        for line in f:\n","            self.json_data.append(json.loads(line))\n","\n","    # special token = ['SPEC'] 을 추가한 부분은 구현되어 있음\n","    special_tokens_dict = {'additional_special_tokens': ['[SPEC]']}\n","    self.tokenizer = tokenizer\n","    self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","  def __len__(self):\n","    return len(self.json_data)\n","\n","  def __getitem__(self, index):\n","    data=self.json_data[index]\n","    answerKey = ord(data[\"answerKey\"])-65 # [A, B, C, D, E]의 answerKey를 [0, 1, 2, 3, 4]로 변환\n","    question = data[\"question\"][\"stem\"]\n","    choicesText=[]\n","    for i in range(len(data[\"question\"][\"choices\"])):\n","        choicesText.append(data[\"question\"][\"choices\"][i][\"text\"])\n","\n","    # Tokenize\n","    '''\n","    배포된 자료를 참고하여 input의 형식을 구성하고\n","    tokenizer를 이용하여 tokenize (max_len=100)\n","    *** question과 choice에 대한 text, 그리고 special token \"[SPEC]\"을 모두 사용하여\n","    하나의 string을 만든 뒤에 그것을 tokenizer에 넣는 것임을 기억하기 ***\n","    '''\n","    string = question\n","    for choice in choicesText:\n","      string += ' [SPEC] ' + choice\n","    input_data = self.tokenizer(string, max_length=100, padding='max_length', truncation=True)\n","\n","    # Conver to tensor\n","    input_ids=torch.IntTensor(input_data[\"input_ids\"])\n","    token_type_ids=torch.IntTensor(input_data[\"token_type_ids\"])\n","    attention_mask=torch.IntTensor(input_data[\"attention_mask\"])\n","\n","    # Store the index(position) of [SPEC] tokens\n","    spec_token_id=self.tokenizer.convert_tokens_to_ids(\"[SPEC]\")\n","    spec_tokens_index = list(filter(lambda x: input_ids[x] == spec_token_id, range(len(input_ids))))\n","    spec_tokens_index = torch.LongTensor(spec_tokens_index)\n","\n","    target=answerKey\n","\n","    return input_ids, token_type_ids, attention_mask, spec_tokens_index, target"]},{"cell_type":"code","execution_count":101,"metadata":{"id":"zELa2_K34MB_","executionInfo":{"status":"ok","timestamp":1693543223393,"user_tz":-540,"elapsed":1,"user":{"displayName":"Wooseok Kim (Wooseok)","userId":"10829587564796253131"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","from transformers import BertModel\n","\n","\n","class myModel(nn.Module):\n","    def __init__(self, tokenizer) -> None:\n","        super().__init__()\n","        self.bert =  BertModel.from_pretrained('bert-base-uncased')\n","        self.bert.resize_token_embeddings(len(tokenizer))\n","        self.linear = nn.Linear(768, 1)\n","\n","    def forward(self, input_ids, token_type_ids, attention_mask, spec_tokens_index):\n","        # bert model에 입력하여 output 도출\n","        output = self.bert(input_ids, token_type_ids, attention_mask)\n","\n","        '''\n","        이중 for문에 대한 설명:\n","        special token의 위치가 batch안의 한 데이터마다 모두 다르기 때문에\n","        해당하는 위치(special token의 position)의 값만 가져오는 부분\n","        (special token의 위치가 batch안의 한 데이터마다 다른 이유는 주어진 question과 answer를 tokenize 했을 때 몇 개의 token으로 tokenize되는지, 그 길이가 다르기 때문)\n","\n","        i는 batch 안의 한 데이터에 접근하기 위함이고\n","        j는 dataset에서 넘겨준 special token의 위치(index)를 한개씩 가져오기 위함\n","        output의 last hidden state에서 각 special token index에 해당하는 값들을 logits list에 append (logits의 shape는 (batch_size*5, dim)이 됨)\n","        '''\n","        logits=[]\n","        for i in range(input_ids.shape[0]): # batch 1개씩\n","            for j in range(len(spec_tokens_index[i])): # choice 1개씩\n","                logits.append(output.last_hidden_state[i,spec_tokens_index[i][j],:])\n","        logits=torch.stack(logits)\n","\n","        # batch processing을 위해 [batch_size*5, dim]의 logits tensor를 한꺼번에 linear에 통과시킴\n","        output = self.linear(logits)\n","\n","        # shape이 [batch_size, 5, dim]이 되도록 reshape\n","        output = output.reshape(-1, 5, 1)\n","\n","        return output"]},{"cell_type":"code","execution_count":127,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bgIO8M3-4MB_","executionInfo":{"status":"ok","timestamp":1693549963133,"user_tz":-540,"elapsed":765287,"user":{"displayName":"Wooseok Kim (Wooseok)","userId":"10829587564796253131"}},"outputId":"e1f85b58-2e54-4275-f002-b7b74caff666"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 28997. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","epoch  0\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 608/608 [03:12<00:00,  3.15it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["train loss 1.611810151291521\n","train acc tensor([0.1987], device='cuda:0')\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 76/76 [00:07<00:00,  9.72it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["test loss 1.6094292292469425\n","test acc tensor([0.1990], device='cuda:0')\n","\n","epoch  1\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 608/608 [03:10<00:00,  3.19it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["train loss 1.6132166801314605\n","train acc tensor([0.2032], device='cuda:0')\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 76/76 [00:08<00:00,  9.10it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["test loss 1.6094363300423873\n","test acc tensor([0.2080], device='cuda:0')\n","\n","epoch  2\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 608/608 [03:10<00:00,  3.19it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["train loss 1.6111124706895728\n","train acc tensor([0.1959], device='cuda:0')\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 76/76 [00:08<00:00,  9.34it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["test loss 1.6094382671933425\n","test acc tensor([0.2105], device='cuda:0')\n","\n","epoch  3\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 608/608 [03:10<00:00,  3.19it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["train loss 1.6108104706202682\n","train acc tensor([0.2029], device='cuda:0')\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 76/76 [00:07<00:00,  9.77it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["test loss 1.6094393918388767\n","test acc tensor([0.2088], device='cuda:0')\n","\n","epoch  4\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 608/608 [03:10<00:00,  3.19it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["train loss 1.6123395481784093\n","train acc tensor([0.1982], device='cuda:0')\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 76/76 [00:08<00:00,  9.46it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["test loss 1.6094387330506976\n","test acc tensor([0.1974], device='cuda:0')\n","\n","epoch  5\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 608/608 [03:10<00:00,  3.19it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["train loss 1.6104097015371448\n","train acc tensor([0.1992], device='cuda:0')\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 76/76 [00:08<00:00,  9.07it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["test loss 1.6094378640777187\n","test acc tensor([0.2064], device='cuda:0')\n","\n","epoch  6\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 608/608 [03:09<00:00,  3.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss 1.6098372610776048\n","train acc tensor([0.1984], device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 76/76 [00:08<00:00,  9.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["test loss 1.6094375691915814\n","test acc tensor([0.2064], device='cuda:0')\n","\n","epoch  7\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 608/608 [03:10<00:00,  3.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss 1.6099116739473844\n","train acc tensor([0.2052], device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 76/76 [00:08<00:00,  8.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["test loss 1.609437854666459\n","test acc tensor([0.2105], device='cuda:0')\n","\n","epoch  8\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 608/608 [03:10<00:00,  3.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss 1.6102877714132007\n","train acc tensor([0.2033], device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 76/76 [00:08<00:00,  8.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["test loss 1.6094379519161426\n","test acc tensor([0.2048], device='cuda:0')\n","\n","epoch  9\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 608/608 [03:09<00:00,  3.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["train loss 1.6097230211292441\n","train acc tensor([0.1985], device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 76/76 [00:08<00:00,  8.79it/s]"]},{"output_type":"stream","name":"stdout","text":["test loss 1.6094381495525962\n","test acc tensor([0.2080], device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import torch\n","from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer\n","\n","# from dataset import myDataset\n","# from model import myModel\n","\n","\n","# Dataset\n","train_data_path = \"/content/drive/MyDrive/nlp-open-tutorial/과제/MultipleChoiceQA_blank/data/train_rand_split.jsonl\" # train dataset의 경로 입력\n","test_data_path = \"/content/drive/MyDrive/nlp-open-tutorial/과제/MultipleChoiceQA_blank/data/dev_rand_split.jsonl\" # test dataset의 경로 입력\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n","train_dataset = myDataset(train_data_path, tokenizer)\n","test_dataset = myDataset(test_data_path, tokenizer)\n","\n","batch_size = 16 # Out of memory error가 뜬다면 batch size를 줄여서 다시 실행시켜보기\n","train_dataloader = DataLoader(train_dataset, batch_size = batch_size, drop_last=True)\n","test_dataloader = DataLoader(test_dataset, batch_size = batch_size, drop_last=True)\n","\n","# Model\n","model = myModel(tokenizer).cuda()\n","\n","# Optimizer/Loss function\n","optimizer = Adam(model.parameters(), lr=0.00001)\n","lf = CrossEntropyLoss()\n","\n","# Train 10 epoch\n","for e in range(10):\n","  print(\"\\nepoch \", e)\n","  epoch_loss = 0\n","  train_correct = 0\n","\n","  model.train()\n","\n","  for batch in tqdm(train_dataloader):\n","    optimizer.zero_grad()\n","\n","    input_ids, token_type_ids, attention_mask, spec_tokens_index, target = batch\n","    input_ids = input_ids.cuda()\n","    token_type_ids = token_type_ids.cuda()\n","    attention_mask = attention_mask.cuda()\n","    spec_tokens_index = spec_tokens_index.cuda()\n","    target = target.cuda()\n","\n","    output = model(input_ids, token_type_ids, attention_mask, spec_tokens_index)\n","    pred_label = torch.argmax(output, dim=1)\n","    train_correct += sum(pred_label == target.reshape(-1,1))\n","\n","    loss = lf(output, target.reshape(-1, 1))\n","\n","    loss.backward()\n","\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","\n","  print(\"train loss\", epoch_loss/len(train_dataloader))\n","  print(\"train acc\", train_correct/len(train_dataset))\n","\n","  # Test at every epoch\n","  test_loss = 0\n","  test_correct = 0\n","\n","  model.eval()\n","  with torch.no_grad():\n","      for batch in tqdm(test_dataloader):\n","        input_ids, token_type_ids, attention_mask, spec_tokens_index, target = batch\n","        input_ids = input_ids.cuda()\n","        token_type_ids = token_type_ids.cuda()\n","        attention_mask = attention_mask.cuda()\n","        spec_tokens_index = spec_tokens_index.cuda()\n","        target = target.cuda()\n","\n","        output = model(input_ids, token_type_ids, attention_mask, spec_tokens_index)\n","        pred_label = torch.argmax(output, dim=1)\n","        test_correct += sum(pred_label == target.reshape(-1,1))\n","\n","        loss = lf(output, target.reshape(-1, 1))\n","\n","        test_loss += loss.item()\n","\n","  print(\"test loss\", test_loss/len(test_dataloader))\n","  print(\"test acc\", test_correct/len(test_dataset))"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4,"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}