{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"od83zh3WUhw4","executionInfo":{"status":"ok","timestamp":1692250551894,"user_tz":-540,"elapsed":17108,"user":{"displayName":"Wooseok Kim (Wooseok)","userId":"10829587564796253131"}},"outputId":"fb9fd22f-c562-4ca4-f2ae-ef7652a8e7a0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["#House price prediction in boston"],"metadata":{"id":"qCgEXYXjUPeZ"}},{"cell_type":"markdown","source":["Pytorch의 MLP를 이용한 집값 예측 모델 구현 (regression)\n","  \n","주요 모듈 목록  \n","**torch.utils.data.Dataset:** 데이터셋을 getitem method에서 한개씩 텐서형태로 반환하도록 구성하는 class  \n","**torch.utils.data.DataLoader:** batch processing을 위해 dataset에서 반환되는 데이터를 n개의 batch_size만큼 묶어서 반환하도록 하는 class  \n","**torch.nn.Module:** 딥러닝 모델을 구현한 class, forward method를 통해 입력된 텐서 데이터를 딥러닝 연산하여 결과 반환  \n","**torch.nn.MSELoss:** loss 연산을 위한 class이며 backward method를 통해 gradient계산  \n","**torch.optim.Adam:** 계산된 gradient를 update해주어 feadient descent를 진행하는 class  "],"metadata":{"id":"--pvfS_VWMij"}},{"cell_type":"markdown","source":["# Dataset class구현  \n","  \n","dataset(Housingdata.csv) : https://www.kaggle.com/datasets/altavish/boston-housing-dataset\n","\n","target: MEDV(집값)  \n","input feature: MEDV를 제외한 모든 값  \n","전체 데이터 약 500개중 400개까지 train data로 사용, 나머지 100개는 test data  \n","\n","각 data를 torch.utils.data.Dataset을 통해 하나씩 load할 수 있는 class구현  \n","이후 torch.utils.data.DataLoader를 통해 batch개씩 변환\n","\n","torch.utils.data.Dataset의 주요 method\n","\n","__ init __: 클래스를 오브젝트로 생성할때 불러와지는 함수, 클래스에서 필요한 인스턴스(데이터셋, 데이터경로)등을 생성  \n","__ len __: 해당 클래스에서 다루는 dataset의 길이를 반환하는 함수  \n","__ getitem __(index): index에 해당하는 데이터 하나를 tensor형태로 반환하는 함수"],"metadata":{"id":"_pyaoU4IYAaw"}},{"cell_type":"code","source":["import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","data_path = \"/content/drive/MyDrive/nlp-open-tutorial/2일차_배포/dataset/HousingData.csv\"\n","batch_size = 4\n","\n","#torch.utils.data.Dataset을 상속하여 Dataset class선언\n","class myDataset(Dataset):\n","  #오브젝트를 선언할때 불러오는 함수, superclass(부모클래스)의 init을 실행해 주어야함\n","  def __init__(self, df_data) -> None:\n","    super().__init__()\n","    self.y = df_data.loc[:,[\"MEDV\"]]\n","    self.x = df_data.drop([\"MEDV\"], axis=1)\n","\n","  #list 형태의 class를 만들때 필수로 사용되는 함수, 전체 길이를 알아야 인덱싱이 가능\n","  def __len__(self):\n","    return len(self.y)\n","\n","  #index에 해당하는 데이터를 반환해주는 함수\n","  def __getitem__(self, index):\n","    data = torch.Tensor(self.x.loc[index,:])\n","    target = torch.FloatTensor(self.y.loc[index,:])\n","    return data, target\n","\n","#data load후 train(400개)/test(나머지)데이터를 분할\n","data_df = pd.read_csv(data_path).dropna()\n","train_data_df = data_df.loc[:400,:].reset_index()\n","test_data_df = data_df.loc[400:,:].reset_index()\n","\n","#train dataset, test dataset을 각각 선언\n","trainDataset = myDataset(train_data_df)\n","testDataset = myDataset(test_data_df)\n","\n","#torch.utils.data.DataLoader을 사용 dataloader 선언, 이를 통해 batch processing\n","trainDataloader = DataLoader(trainDataset, batch_size)\n","testDataloader = DataLoader(testDataset, batch_size)\n","\n","#잘 작동하는지 test\n","for i in trainDataset:\n","  print(\"dataset test\")\n","  print(i)\n","  break\n","\n","#잘 작동하는지 test\n","for i in trainDataloader:\n","  print(\"data loader test\")\n","  data = i[0]\n","  target = i[1]\n","\n","  print(\"data\")\n","  print(data)\n","  print(data.shape)\n","  print(\"\\ntarget\")\n","  print(target)\n","  print(target.shape)\n","  break\n"],"metadata":{"id":"GgwV6nIIUapc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692252068416,"user_tz":-540,"elapsed":3,"user":{"displayName":"Wooseok Kim (Wooseok)","userId":"10829587564796253131"}},"outputId":"184809aa-be62-4d3f-8b11-e47c7e7afaee"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["dataset test\n","(tensor([0.0000e+00, 6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n","        6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02, 1.5300e+01,\n","        3.9690e+02, 4.9800e+00]), tensor([24.]))\n","data loader test\n","data\n","tensor([[0.0000e+00, 6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n","         6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02, 1.5300e+01,\n","         3.9690e+02, 4.9800e+00],\n","        [1.0000e+00, 2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n","         6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01,\n","         3.9690e+02, 9.1400e+00],\n","        [2.0000e+00, 2.7290e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n","         7.1850e+00, 6.1100e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01,\n","         3.9283e+02, 4.0300e+00],\n","        [3.0000e+00, 3.2370e-02, 0.0000e+00, 2.1800e+00, 0.0000e+00, 4.5800e-01,\n","         6.9980e+00, 4.5800e+01, 6.0622e+00, 3.0000e+00, 2.2200e+02, 1.8700e+01,\n","         3.9463e+02, 2.9400e+00]])\n","torch.Size([4, 14])\n","\n","target\n","tensor([[24.0000],\n","        [21.6000],\n","        [34.7000],\n","        [33.4000]])\n","torch.Size([4, 1])\n"]}]},{"cell_type":"markdown","source":["# Deep Learning Model 구현  \n","torch.nn.Module을 이용하여 모델 구현\n","\n","1st hidden layer의 feature는 100개  \n","2nd hidden layer의 feature는 10개인 모델을 구현한다.  \n","\n","torch.nn.Linear: perceptron의 weighted sum과 같이 linaer regression연산을 하는 calss  \n","torch.nn.ReLU: ReLU activation function을 수행하는 class  \n","\n","torch.nn.Module의 주요 method  \n","__ init __: 클래스를 오브젝트로 생성할때 불러와지는 함수, 클래스에서 필요한 인스턴스(사용할 deep learning layer, activation function, 등)등을 생성  \n","__ forward __(data): 입력받은 data를 딥러닝 모델을 통해 결과를 예측하여 반환하는 class  \n","\n","**각 딥러닝 레이어 연산 중 차원수를 확인하고 잘 맞춰줄 것**  \n","참고 document(해당 사이트의 shape를 확인하고 tensor형태 결정)  \n","nn.Linear: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html  \n","nn.ReLU: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html"],"metadata":{"id":"n7j-zH_mfTbY"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"pxJqIF7cW-lT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692252398345,"user_tz":-540,"elapsed":5,"user":{"displayName":"Wooseok Kim (Wooseok)","userId":"10829587564796253131"}},"outputId":"4b701e1f-8584-4b63-fa44-d620a4e8b865"},"outputs":[{"output_type":"stream","name":"stdout","text":["model test\n","input data\n","tensor([[0.0000e+00, 6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01,\n","         6.5750e+00, 6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02, 1.5300e+01,\n","         3.9690e+02, 4.9800e+00],\n","        [1.0000e+00, 2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n","         6.4210e+00, 7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01,\n","         3.9690e+02, 9.1400e+00],\n","        [2.0000e+00, 2.7290e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01,\n","         7.1850e+00, 6.1100e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01,\n","         3.9283e+02, 4.0300e+00],\n","        [3.0000e+00, 3.2370e-02, 0.0000e+00, 2.1800e+00, 0.0000e+00, 4.5800e-01,\n","         6.9980e+00, 4.5800e+01, 6.0622e+00, 3.0000e+00, 2.2200e+02, 1.8700e+01,\n","         3.9463e+02, 2.9400e+00]])\n","torch.Size([4, 14])\n","\n","output predict\n","tensor([[-3.4231],\n","        [-2.4632],\n","        [-2.8585],\n","        [-3.2171]], grad_fn=<AddmmBackward0>)\n","torch.Size([4, 1])\n","\n","ground thruth\n","tensor([[24.0000],\n","        [21.6000],\n","        [34.7000],\n","        [33.4000]])\n","torch.Size([4, 1])\n"]}],"source":["from torch import nn\n","\n","#딥러닝 모델을 작성하기위한 모듈\n","class myModel(nn.Module):\n","  #오브젝트를 선언할 때 불러와지는 함수 일반적으로 모델에서 사용될 각 레이어들이 포함됨\n","  def __init__(self) -> None:\n","      super().__init__()\n","      #input_feature:14,  1st_hidden: 100, 2nd_hidden: 10, output: 1의 형태에 맞는 linear layer들을 선언, activation으로 Relu사용\n","      self.linear = nn.Linear(14, 100)\n","      self.linear2 = nn.Linear(100, 10)\n","      self.linear3 = nn.Linear(10,1)\n","      self.relu = nn.ReLU()\n","\n","  #데이터를 입력받고 딥러닝 연산후 결과를 반환하는 함수\n","  def forward(self, x):\n","      x = self.linear(x)\n","      x = self.relu(x)\n","      x = self.linear2(x)\n","      x = self.relu(x)\n","      x = self.linear3(x)\n","\n","      return x\n","\n","#작성한 모델 선언\n","model = myModel()\n","\n","#잘 작동하는지 test\n","for i in trainDataloader:\n","  print(\"model test\")\n","  data = i[0]\n","  target = i[1]\n","\n","  print(\"input data\")\n","  print(data)\n","  print(data.shape)\n","  print(\"\\noutput predict\")\n","  print(model(data))\n","  print(model(data).shape)\n","  print(\"\\nground thruth\")\n","  print(target)\n","  print(target.shape)\n","  break\n"]},{"cell_type":"markdown","source":["# 작성한 dataset과 model을 이용하여 deep learning process 구현\n","\n","pytorch 딥러닝 프로세스\n","1. dataset, model선언\n","2. dataset과 model을 통한 결과 예측\n","3. 예측된 결과를 통해 **loss연산** 및 **loss.backward**\n","4. **optimizer.step()**를 사용하여 graident update\n","\n","주요 오브젝트  \n","torch.nn.MSELoss: 예측값과 정답을 통해 MSE값을 반환하는 class  \n","torch.optim.Adam: Adam optimizer를 통해 gradient update를 수행하는 class\n","\n","\n","각 오브젝트의 입력과 선언은 다음 doc 참조:  \n","MSELoss: https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html  \n","Adam: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html"],"metadata":{"id":"MvVKNpwAoymc"}},{"cell_type":"code","source":["from torch.optim import Adam\n","from torch.nn import MSELoss\n","\n","#학습을 위한 optimizer와 loss function 설정\n","lf = MSELoss()\n","optimizer = Adam(model.parameters(), lr=0.001)\n","\n","#100번의 epoch을 실행\n","for e in range(100):\n","  print(\"\\nepoch \", e)\n","  epoch_loss = 0\n","\n","  #선언한 model을 학습 가능한 상태로 변경\n","  model.train()\n","\n","  #모든 train data에 대해서 학습\n","  for i in trainDataloader:\n","    #매 배치에 대한 gradient 계산 이전에 optimizer에 저장된 이전 batch의 gradient를 삭제(초기화)\n","    optimizer.zero_grad()\n","\n","    data = i[0]\n","    target = i[1]\n","\n","    #결과 도출\n","    output = model(data)\n","\n","    #loss 계산 (MSE는 output과 target의 shape이 일치하도록)\n","    loss = lf(output, target)\n","\n","    #backpropagation\n","    loss.backward()\n","\n","    #gradient update\n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","\n","  print(\"train loss\", epoch_loss/len(trainDataloader))\n","\n","  #model이 학습되지 않는 상태로 변경\n","  model.eval()\n","  test_loss = 0\n","\n","  #gradient를 계산하지 않도록 하여 cost낭비 방지\n","  with torch.no_grad():\n","    #모든 test dataset에 대해서 결과연산\n","    for i in testDataloader:\n","      data = i[0]\n","      target = i[1]\n","\n","      output = model(data)\n","\n","      loss = lf(output, target)\n","      test_loss += loss.item()\n","\n","  print(\"test loss\", test_loss/len(testDataloader))\n","\n"],"metadata":{"id":"enENUuVFo4VN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692252426127,"user_tz":-540,"elapsed":24881,"user":{"displayName":"Wooseok Kim (Wooseok)","userId":"10829587564796253131"}},"outputId":"980a258d-c76c-405a-ca62-14fef8895b7d"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","epoch  0\n","train loss 126.8013574443286\n","test loss 65.75581132797967\n","\n","epoch  1\n","train loss 84.81891114500505\n","test loss 92.38695117405483\n","\n","epoch  2\n","train loss 71.48581759160078\n","test loss 125.60539345514206\n","\n","epoch  3\n","train loss 80.74466441401952\n","test loss 121.91030665806362\n","\n","epoch  4\n","train loss 88.68818681904033\n","test loss 145.44529015677315\n","\n","epoch  5\n","train loss 90.7303034685835\n","test loss 139.93490155537924\n","\n","epoch  6\n","train loss 89.55470282518411\n","test loss 146.6666997273763\n","\n","epoch  7\n","train loss 84.59585257459291\n","test loss 140.08889843168714\n","\n","epoch  8\n","train loss 80.95090159811551\n","test loss 147.8440024058024\n","\n","epoch  9\n","train loss 72.70341113926489\n","test loss 100.95087342035202\n","\n","epoch  10\n","train loss 66.12874949657464\n","test loss 150.00431242443267\n","\n","epoch  11\n","train loss 56.744451739365545\n","test loss 149.63891092936197\n","\n","epoch  12\n","train loss 51.848193124879764\n","test loss 142.11482002621605\n","\n","epoch  13\n","train loss 49.32077111850811\n","test loss 154.69649505615234\n","\n","epoch  14\n","train loss 46.363729919813856\n","test loss 146.03222138541085\n","\n","epoch  15\n","train loss 44.64887489850008\n","test loss 156.53338196164086\n","\n","epoch  16\n","train loss 44.88434783454183\n","test loss 152.3203956059047\n","\n","epoch  17\n","train loss 44.84882332460035\n","test loss 151.60056096031553\n","\n","epoch  18\n","train loss 45.28798745780052\n","test loss 153.37717301504952\n","\n","epoch  19\n","train loss 46.19014961055562\n","test loss 152.2381502787272\n","\n","epoch  20\n","train loss 46.00593324250813\n","test loss 151.3659284682501\n","\n","epoch  21\n","train loss 46.86277763903895\n","test loss 150.67613874162947\n","\n","epoch  22\n","train loss 47.364903560167626\n","test loss 136.2947353181385\n","\n","epoch  23\n","train loss 47.47636955146548\n","test loss 129.72681735810778\n","\n","epoch  24\n","train loss 46.798291116575655\n","test loss 123.58927118210565\n","\n","epoch  25\n","train loss 46.07791718501079\n","test loss 121.25348726908366\n","\n","epoch  26\n","train loss 42.116376903992666\n","test loss 119.28237406412761\n","\n","epoch  27\n","train loss 45.91442479966562\n","test loss 122.140532993135\n","\n","epoch  28\n","train loss 41.57689445833616\n","test loss 123.59340613228935\n","\n","epoch  29\n","train loss 38.507962859129606\n","test loss 119.89866865248908\n","\n","epoch  30\n","train loss 36.48189560748354\n","test loss 121.48189998808361\n","\n","epoch  31\n","train loss 35.03016495855549\n","test loss 117.27876318068732\n","\n","epoch  32\n","train loss 32.839317838979674\n","test loss 110.861023130871\n","\n","epoch  33\n","train loss 31.488989740232878\n","test loss 100.93169384910946\n","\n","epoch  34\n","train loss 30.604095577439175\n","test loss 91.67865753173828\n","\n","epoch  35\n","train loss 29.897350843948654\n","test loss 83.61286235990978\n","\n","epoch  36\n","train loss 29.29065877878213\n","test loss 78.70833596729096\n","\n","epoch  37\n","train loss 28.468729837031304\n","test loss 72.67867288135346\n","\n","epoch  38\n","train loss 28.191840426076816\n","test loss 71.77133887154716\n","\n","epoch  39\n","train loss 27.76181232627434\n","test loss 69.27836990356445\n","\n","epoch  40\n","train loss 26.59065475192251\n","test loss 73.15994989304315\n","\n","epoch  41\n","train loss 26.17378785414032\n","test loss 75.41656412397113\n","\n","epoch  42\n","train loss 25.81908266981946\n","test loss 75.47806213015602\n","\n","epoch  43\n","train loss 25.556682777932927\n","test loss 80.71295806339809\n","\n","epoch  44\n","train loss 25.404985181892975\n","test loss 78.30282238551548\n","\n","epoch  45\n","train loss 25.121230808994437\n","test loss 83.42856107439313\n","\n","epoch  46\n","train loss 25.044420389434958\n","test loss 81.06200645083473\n","\n","epoch  47\n","train loss 24.766534320161313\n","test loss 83.60066663651239\n","\n","epoch  48\n","train loss 24.621110557000847\n","test loss 80.92827542622884\n","\n","epoch  49\n","train loss 24.4417531626134\n","test loss 81.1528803507487\n","\n","epoch  50\n","train loss 24.341479618715333\n","test loss 80.44273444584438\n","\n","epoch  51\n","train loss 24.166106002617486\n","test loss 80.31832876659576\n","\n","epoch  52\n","train loss 24.088227935984165\n","test loss 78.48537331535702\n","\n","epoch  53\n","train loss 24.000822169494025\n","test loss 78.32397470020112\n","\n","epoch  54\n","train loss 23.86357462707954\n","test loss 72.71907779148647\n","\n","epoch  55\n","train loss 24.16446465329279\n","test loss 64.3959420522054\n","\n","epoch  56\n","train loss 23.556458328343645\n","test loss 73.1660229365031\n","\n","epoch  57\n","train loss 23.05812046516545\n","test loss 80.93919685908726\n","\n","epoch  58\n","train loss 22.997945170613782\n","test loss 76.16058058965774\n","\n","epoch  59\n","train loss 22.11035494072528\n","test loss 74.13848722548713\n","\n","epoch  60\n","train loss 22.085585927284217\n","test loss 77.63919271741595\n","\n","epoch  61\n","train loss 21.438099600091764\n","test loss 79.01610165550595\n","\n","epoch  62\n","train loss 21.233719818954228\n","test loss 84.80283573695591\n","\n","epoch  63\n","train loss 21.20370595515529\n","test loss 89.6842420668829\n","\n","epoch  64\n","train loss 21.21726463037201\n","test loss 93.47004686083112\n","\n","epoch  65\n","train loss 20.971151173303397\n","test loss 93.56174187433152\n","\n","epoch  66\n","train loss 21.061180093243152\n","test loss 93.17019603365944\n","\n","epoch  67\n","train loss 20.892814030752906\n","test loss 97.30638962700253\n","\n","epoch  68\n","train loss 21.38055577232868\n","test loss 86.56875342414493\n","\n","epoch  69\n","train loss 23.21509563847433\n","test loss 70.2002347310384\n","\n","epoch  70\n","train loss 23.98577828724173\n","test loss 55.19746076493036\n","\n","epoch  71\n","train loss 24.914072063904776\n","test loss 57.42190336045765\n","\n","epoch  72\n","train loss 31.69049563306042\n","test loss 53.28961640312558\n","\n","epoch  73\n","train loss 33.4861314081693\n","test loss 75.26703264599755\n","\n","epoch  74\n","train loss 26.76833912051177\n","test loss 124.99916308266776\n","\n","epoch  75\n","train loss 26.95723438319526\n","test loss 118.48750009990874\n","\n","epoch  76\n","train loss 26.50545715399181\n","test loss 91.63769485836937\n","\n","epoch  77\n","train loss 27.51227087084251\n","test loss 89.52487604958671\n","\n","epoch  78\n","train loss 27.794312662716152\n","test loss 106.00795409792946\n","\n","epoch  79\n","train loss 26.16253028376193\n","test loss 124.30044682820638\n","\n","epoch  80\n","train loss 23.23221462248247\n","test loss 134.26415625072661\n","\n","epoch  81\n","train loss 23.944670105659508\n","test loss 127.32480285281227\n","\n","epoch  82\n","train loss 26.94256982433645\n","test loss 113.4345437912714\n","\n","epoch  83\n","train loss 28.352959268455265\n","test loss 120.17982868921189\n","\n","epoch  84\n","train loss 25.67856776865223\n","test loss 141.08047703334265\n","\n","epoch  85\n","train loss 22.618807517275027\n","test loss 142.97214980352493\n","\n","epoch  86\n","train loss 23.936851259839685\n","test loss 128.87799944196428\n","\n","epoch  87\n","train loss 23.98623921675018\n","test loss 126.6117159979684\n","\n","epoch  88\n","train loss 22.57141201858279\n","test loss 132.5067789895194\n","\n","epoch  89\n","train loss 23.265887578642822\n","test loss 108.11995038532075\n","\n","epoch  90\n","train loss 20.879086066273196\n","test loss 112.46262840997605\n","\n","epoch  91\n","train loss 22.36554740275009\n","test loss 114.17412535349528\n","\n","epoch  92\n","train loss 21.093136001236832\n","test loss 97.49592385973249\n","\n","epoch  93\n","train loss 20.818523656341092\n","test loss 97.75599743071056\n","\n","epoch  94\n","train loss 20.946001223371\n","test loss 80.21413012913295\n","\n","epoch  95\n","train loss 19.462357721562626\n","test loss 88.42179366520473\n","\n","epoch  96\n","train loss 20.583239394086824\n","test loss 79.49489143916539\n","\n","epoch  97\n","train loss 19.052364225817634\n","test loss 75.19825426737468\n","\n","epoch  98\n","train loss 19.572857903151572\n","test loss 80.88838041396369\n","\n","epoch  99\n","train loss 18.804268322790726\n","test loss 84.47468516940162\n"]}]}]}