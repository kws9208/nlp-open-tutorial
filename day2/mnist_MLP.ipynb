{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"jgKuWMxk3lCZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692257947549,"user_tz":-540,"elapsed":23844,"user":{"displayName":"Wooseok Kim (Wooseok)","userId":"10829587564796253131"}},"outputId":"e47ed39c-0676-42a6-ffc5-40c75031cfb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["#MNIST class prediction"],"metadata":{"id":"LbNjEAEBJCvx"}},{"cell_type":"markdown","source":["mnist는 숫자를 손으로 그린 그림에 대한 데이터  \n","각 그림을 보고 어떠한 숫자가 적혀있는지 classification 하는 딥러닝 모델 구축  \n","multi class classification은 강의자료의 appendix 참조  \n","\n","주요 모듈 목록  \n","**torch.utils.data.Dataset:** 데이터셋을 getitem method에서 한개씩 텐서형태로 반환하도록 구성하는 class  \n","**torch.utils.data.DataLoader:** batch processing을 위해 dataset에서 반환되는 데이터를 n개의 batch_size만큼 묶어서 반환하도록 하는 class  \n","**torch.nn.Module:** 딥러닝 모델을 구현한 class, forward method를 통해 입력된 텐서 데이터를 딥러닝 연산하여 결과 반환  \n","**torch.nn.CrossEntropyLoss:** loss 연산을 위한 class이며 backward method를 통해 gradient계산  \n","**torch.optim.Adam:** 계산된 gradient를 update해주어 feadient descent를 진행하는 class  "],"metadata":{"id":"UQc-59mMJGvr"}},{"cell_type":"markdown","source":["# Dataset class구현  \n","  \n","dataset(fashion-mnist) : https://www.kaggle.com/datasets/zalando-research/fashionmnist?resource=download\n","\n","target: label(적힌 숫자)  \n","input feature: pixel1 ~ pixel 784까지의 명도\n","train/test file이 구분되어있음  \n","\n","각 data를 torch.utils.data.Dataset을 통해 하나씩 load할 수 있는 class구현  \n","이후 torch.utils.data.DataLoader를 통해 batch개씩 변환\n","\n","torch.utils.data.Dataset의 주요 method\n","__ init __: 클래스를 오브젝트로 생성할때 불러와지는 함수, 클래스에서 필요한 인스턴스(데이터셋, 데이터경로)등을 생성  \n","__ len __: 해당 클래스에서 다루는 dataset의 길이를 반환하는 함수  \n","__ getitem __(index): index에 해당하는 데이터 하나를 tensor형태로 반환하는 함수"],"metadata":{"id":"-Dui_MxkJqXY"}},{"cell_type":"code","source":["import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","\n","train_data_path = \"/content/drive/MyDrive/nlp-open-tutorial/2일차_배포/dataset/fashion-mnist_train.csv\"\n","test_data_path = \"/content/drive/MyDrive/nlp-open-tutorial/2일차_배포/dataset/fashion-mnist_test.csv\"\n","batch_size = 64\n","\n","#torch.utils.data.Dataset을 상속하여 Dataset class선언\n","class myDataset(Dataset):\n","  #오브젝트를 선언할때 불러오는 함수, superclass(부모클래스)의 init을 실행해 주어야함\n","  def __init__(self, df_data) -> None:\n","    super().__init__()\n","    self.y = df_data.loc[:,[\"label\"]]\n","    self.x = df_data.drop([\"label\"], axis=1)\n","\n","  #list 형태의 class를 만들때 필수로 사용되는 함수, 전체 길이를 알아야 인덱싱이 가능\n","  def __len__(self):\n","    return len(self.y)\n","\n","  #index에 해당하는 데이터를 반환해주는 함수\n","  def __getitem__(self, index):\n","    data = torch.Tensor(self.x.loc[index,:])\n","    target = torch.LongTensor(self.y.loc[index,:])\n","    return data, target\n","\n","#data load후 분할\n","train_data_df = pd.read_csv(train_data_path).dropna().loc[:5000,:]\n","test_data_df = pd.read_csv(test_data_path).dropna().loc[:1000,:]\n","\n","\n","#train dataset, test dataset을 각각 선언\n","trainDataset = myDataset(train_data_df)\n","testDataset = myDataset(test_data_df)\n","\n","#torch.utils.data.DataLoader을 사용 dataloader 선언, 이를 통해 batch processing\n","trainDataloader = DataLoader(trainDataset, batch_size)\n","testDataloader = DataLoader(testDataset, batch_size)\n","\n","#잘 작동하는지 test\n","for i in trainDataset:\n","  print(\"dataset test\")\n","  print(i)\n","  break\n","\n","#잘 작동하는지 test\n","for i in trainDataloader:\n","  print(\"data loader test\")\n","  data = i[0]\n","  target = i[1]\n","\n","  print(\"data\")\n","  print(data)\n","  print(data.shape)\n","  print(\"\\ntarget\")\n","  print(target)\n","  print(target.shape)\n","  break\n"],"metadata":{"id":"SPqN84AJJGH8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692257959772,"user_tz":-540,"elapsed":12227,"user":{"displayName":"Wooseok Kim (Wooseok)","userId":"10829587564796253131"}},"outputId":"410f2681-53bf-4f1f-9892-d0211464f6d6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["dataset test\n","(tensor([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   0.,   0.,   4.,   0.,   0.,   0.,   0.,   0.,  62.,  61.,\n","         21.,  29.,  23.,  51., 136.,  61.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  88.,\n","        201., 228., 225., 255., 115.,  62., 137., 255., 235., 222., 255., 135.,\n","          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,  47., 252., 234., 238., 224., 215., 215., 229., 108., 180., 207.,\n","        214., 224., 231., 249., 254.,  45.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   1.,   0.,   0., 214., 222., 210., 213., 224., 225., 217.,\n","        220., 254., 233., 219., 221., 217., 223., 221., 240., 254.,   0.,   0.,\n","          1.,   0.,   0.,   0.,   1.,   0.,   0.,   0., 128., 237., 207., 224.,\n","        224., 207., 216., 214., 210., 208., 211., 221., 208., 219., 213., 226.,\n","        211., 237., 150.,   0.,   0.,   0.,   0.,   0.,   0.,   2.,   0.,   0.,\n","        237., 222., 215., 207., 210., 212., 213., 206., 214., 213., 214., 213.,\n","        210., 215., 214., 206., 199., 218., 255.,  13.,   0.,   2.,   0.,   0.,\n","          0.,   4.,   0.,  85., 228., 210., 218., 200., 211., 208., 203., 215.,\n","        210., 209., 209., 210., 213., 211., 210., 217., 206., 213., 231., 175.,\n","          0.,   0.,   0.,   0.,   0.,   0.,   0., 217., 224., 215., 206., 205.,\n","        204., 217., 230., 222., 215., 224., 233., 228., 232., 228., 224., 207.,\n","        212., 215., 213., 229.,  31.,   0.,   4.,   0.,   1.,   0.,  21., 225.,\n","        212., 212., 203., 211., 225., 193., 139., 136., 195., 147., 156., 139.,\n","        128., 162., 197., 223., 207., 220., 213., 232., 177.,   0.,   0.,   0.,\n","          0.,   0., 123., 226., 207., 211., 209., 205., 228., 158.,  90., 103.,\n","        186., 138., 100., 121., 147., 158., 183., 226., 208., 214., 209., 216.,\n","        255.,  13.,   0.,   1.,   0.,   0., 226., 219., 202., 208., 206., 205.,\n","        216., 184., 156., 150., 193., 170., 164., 168., 188., 186., 200., 219.,\n","        216., 213., 213., 211., 233., 148.,   0.,   0.,   0.,  45., 227., 204.,\n","        214., 211., 218., 222., 221., 230., 229., 221., 213., 224., 233., 226.,\n","        220., 219., 221., 224., 223., 217., 210., 218., 213., 254.,   0.,   0.,\n","          0., 157., 226., 203., 207., 211., 209., 215., 205., 198., 207., 208.,\n","        201., 201., 197., 203., 205., 210., 207., 213., 214., 214., 214., 213.,\n","        208., 234., 107.,   0.,   0., 235., 213., 204., 211., 210., 209., 213.,\n","        202., 197., 204., 215., 217., 213., 212., 210., 206., 212., 203., 211.,\n","        218., 215., 214., 208., 209., 222., 230.,   0.,  52., 255., 207., 200.,\n","        208., 213., 210., 210., 208., 207., 202., 201., 209., 216., 216., 216.,\n","        216., 214., 212., 205., 215., 201., 228., 208., 214., 212., 218.,  25.,\n","        118., 217., 201., 206., 208., 213., 208., 205., 206., 210., 211., 202.,\n","        199., 207., 208., 209., 210., 207., 210., 210., 245., 139., 119., 255.,\n","        202., 203., 236., 114., 171., 238., 212., 203., 220., 216., 217., 209.,\n","        207., 205., 210., 211., 206., 204., 206., 209., 211., 215., 210., 206.,\n","        221., 242.,   0., 224., 234., 230., 181.,  26.,  39., 145., 201., 255.,\n","        157., 115., 250., 200., 207., 206., 207., 213., 216., 206., 205., 206.,\n","        207., 206., 215., 207., 221., 238.,   0.,   0., 188.,  85.,   0.,   0.,\n","          0.,   0.,   0.,  31.,   0., 129., 253., 190., 207., 208., 208., 208.,\n","        209., 211., 211., 209., 209., 209., 212., 201., 226., 165.,   0.,   0.,\n","          0.,   0.,   0.,   0.,   2.,   0.,   0.,   0.,   0.,  89., 254., 199.,\n","        199., 192., 196., 198., 199., 201., 202., 203., 204., 203., 203., 200.,\n","        222., 155.,   0.,   3.,   3.,   3.,   2.,   0.,   0.,   0.,   1.,   5.,\n","          0.,   0., 255., 218., 226., 232., 228., 224., 222., 220., 219., 219.,\n","        217., 221., 220., 212., 236.,  95.,   0.,   2.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   0.,   0.,   0.,   0., 155., 194., 168., 170., 171., 173.,\n","        173., 179., 177., 175., 172., 171., 167., 161., 180.,   0.,   0.,   1.,\n","          0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n","          0.,   0.,   0.,   0.]), tensor([2]))\n","data loader test\n","data\n","tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","torch.Size([64, 784])\n","\n","target\n","tensor([[2],\n","        [9],\n","        [6],\n","        [0],\n","        [3],\n","        [4],\n","        [4],\n","        [5],\n","        [4],\n","        [8],\n","        [0],\n","        [8],\n","        [9],\n","        [0],\n","        [2],\n","        [2],\n","        [9],\n","        [3],\n","        [3],\n","        [3],\n","        [8],\n","        [7],\n","        [4],\n","        [4],\n","        [0],\n","        [4],\n","        [4],\n","        [8],\n","        [7],\n","        [1],\n","        [5],\n","        [0],\n","        [5],\n","        [3],\n","        [2],\n","        [7],\n","        [3],\n","        [4],\n","        [2],\n","        [1],\n","        [6],\n","        [0],\n","        [9],\n","        [6],\n","        [0],\n","        [5],\n","        [6],\n","        [7],\n","        [7],\n","        [2],\n","        [5],\n","        [2],\n","        [2],\n","        [4],\n","        [1],\n","        [4],\n","        [9],\n","        [8],\n","        [3],\n","        [4],\n","        [5],\n","        [5],\n","        [6],\n","        [3]])\n","torch.Size([64, 1])\n"]}]},{"cell_type":"markdown","source":["# Deep Learning Model 구현  \n","\n","10개의 label의 확률을 예측해야하는 모델  \n","그렇기에 output layer의 최종 결과가 10차원의 vector가 되어야함\n","\n","torch.nn.Module을 이용하여 모델 구현  \n","1st hidden layer의 feature는 10개  \n","2nd hidden layer의 feature는 100개인 모델을 구현한다.  \n","\n","torch.nn.Linear: perceptron의 weighted sum과 같이 linaer regression연산을 하는 calss  \n","torch.nn.ReLU: ReLU activation function을 수행하는 class  \n","\n","torch.nn.Module의 주요 method  \n","__ init __: 클래스를 오브젝트로 생성할때 불러와지는 함수, 클래스에서 필요한 인스턴스(사용할 deep learning layer, activation function, 등)등을 생성  \n","__ forward __(data): 입력받은 data를 딥러닝 모델을 통해 결과를 예측하여 반환하는 class  \n","\n","**각 딥러닝 레이어 연산 중 차원수를 확인하고 잘 맞춰줄 것**  \n","참고 document(해당 사이트의 shape를 확인하고 tensor형태 결정)  \n","nn.Linear: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html  \n","nn.ReLU: https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html"],"metadata":{"id":"xrM7jz2NLunt"}},{"cell_type":"code","source":["from torch import nn\n","\n","#딥러닝 모델을 작성하기위한 모듈\n","class myModel(nn.Module):\n","  #오브젝트를 선언할 때 불러와지는 함수 일반적으로 이번 모델에서 사용될 각 레이어들이 포함됨\n","  def __init__(self) -> None:\n","      super().__init__()\n","      self.linear = nn.Linear(784, 10)\n","      self.linear2 = nn.Linear(10, 100)\n","      self.linear3 = nn.Linear(100, 10)\n","      self.relu = nn.ReLU()\n","\n","  #데이터를 입력받고 딥러닝 연산후 결과를 반환하는 함수\n","  def forward(self, x):\n","      x = self.linear(x)\n","      x = self.relu(x)\n","      x = self.linear2(x)\n","      x = self.relu(x)\n","      x = self.linear3(x)\n","      return x\n","\n","#작성한 모델 선언\n","model = myModel()\n","\n","#잘 작동하는지 test\n","for i in trainDataloader:\n","  print(\"model test\")\n","  data = i[0]\n","  target = i[1]\n","\n","  print(\"input data\")\n","  print(data)\n","  print(data.shape)\n","  print(\"\\noutput predict\")\n","  print(model(data))\n","  print(model(data).shape)\n","  print(\"\\nground thruth\")\n","  print(target)\n","  print(target.shape)\n","  break\n"],"metadata":{"id":"1HaDnh-TMQG1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692257959772,"user_tz":-540,"elapsed":19,"user":{"displayName":"Wooseok Kim (Wooseok)","userId":"10829587564796253131"}},"outputId":"888495bf-3291-4f77-a46d-205ea4c7bd87"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["model test\n","input data\n","tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        ...,\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.],\n","        [0., 0., 0.,  ..., 0., 0., 0.]])\n","torch.Size([64, 784])\n","\n","output predict\n","tensor([[ 1.0968e+00, -2.6694e+01, -9.5882e-01, -1.7280e+00,  8.5507e+00,\n","         -5.2811e+00,  4.4647e+00, -6.9400e+00, -1.7173e+01,  1.0489e+01],\n","        [-3.5807e+00, -5.9574e+00, -4.9482e+00,  2.5328e+00, -2.3187e+00,\n","         -4.7688e+00,  3.2444e+00, -3.3278e+00, -7.9297e+00,  1.1053e+01],\n","        [-3.9013e+00, -1.3315e+01, -5.8114e+00, -4.5361e+00, -3.3558e+00,\n","         -1.3393e+01,  1.5805e+01, -8.8826e+00, -1.8060e+01,  7.4419e+00],\n","        [-1.7865e+00,  5.2013e-02, -1.0224e+01,  5.3646e-01,  4.5045e+00,\n","         -2.5247e+00, -1.4849e+00,  4.1400e+00, -7.8903e+00,  1.5217e+01],\n","        [-1.1252e+00, -2.1223e+01, -7.6173e+00,  2.1989e+00,  6.8870e+00,\n","         -1.5786e+00,  6.4281e+00, -1.7729e+00, -1.6691e+01,  1.3658e+01],\n","        [-2.8439e+00, -4.7259e+00, -2.2567e+00, -2.2460e+00, -4.9936e+00,\n","         -8.7796e+00,  1.0905e+01, -2.2242e+00, -9.9559e+00,  7.8608e+00],\n","        [-4.8294e+00, -2.0376e+01, -3.6884e+00,  5.9589e-01, -1.4779e+00,\n","         -1.0550e+01,  1.1004e+01, -2.0184e+00, -1.3342e+01,  1.2565e+01],\n","        [ 4.6523e-01, -2.8512e+00, -1.3866e+00, -1.9545e+00, -5.5245e-01,\n","         -2.0694e-01,  6.6458e-01, -4.6110e+00, -4.4234e+00,  3.6383e+00],\n","        [-5.7762e+00, -2.1864e+01, -5.8451e+00, -7.2678e+00, -8.3068e-01,\n","         -1.2184e+01,  1.9487e+01, -9.7035e+00, -2.1850e+01,  6.2981e+00],\n","        [-5.5655e-01, -1.0767e+01, -4.9238e+00, -4.2114e+00,  1.1493e+00,\n","         -4.1573e+00,  8.1090e+00, -9.3730e+00, -1.0177e+01,  3.9962e+00],\n","        [-5.4150e+00, -5.9344e+00, -1.2486e+01,  2.3193e+00, -3.7978e+00,\n","         -8.3675e+00,  6.9031e+00, -5.4722e-01, -1.1867e+01,  1.7232e+01],\n","        [-3.0033e-01, -1.8353e+01, -1.2270e+01,  1.7684e+00,  6.2939e+00,\n","          5.3034e+00,  3.2210e+00, -8.6083e-01, -1.7625e+01,  9.7893e+00],\n","        [-4.0796e+00, -2.9318e+00, -1.5629e+00, -5.9580e-01, -2.5750e+00,\n","         -7.6200e+00,  5.5922e+00, -2.4118e+00, -7.0929e+00,  1.1222e+01],\n","        [ 4.1934e+00, -2.0619e-01, -9.6416e+00, -6.6062e+00, -6.7188e+00,\n","          2.1478e+00,  2.7747e+00, -6.1205e+00, -1.3911e+01,  1.3639e+01],\n","        [-1.3601e+00, -7.9638e+00, -1.7038e+00, -5.0643e+00, -1.1567e+00,\n","         -4.2576e+00,  1.0239e+01, -5.0369e+00, -9.6052e+00,  2.7103e+00],\n","        [-1.4141e+00, -1.8460e+00, -6.0957e+00,  8.1351e-01, -1.8833e+00,\n","          2.9822e-01,  2.4030e+00, -4.3668e+00, -4.8280e+00,  5.0340e+00],\n","        [-1.2771e+01, -1.2311e+01, -1.1913e+01,  9.9965e+00, -3.1219e-01,\n","         -9.7186e+00,  4.4206e+00, -6.3697e+00, -1.6306e+01,  2.6947e+01],\n","        [-4.3361e-01, -2.7121e+01, -1.0947e+01,  4.6529e-01,  7.6351e+00,\n","          2.7065e+00,  8.2905e+00, -1.5284e+00, -2.1217e+01,  1.2982e+01],\n","        [-7.7100e+00, -1.9943e+01, -6.1524e+00, -1.8244e-02, -5.1485e+00,\n","         -1.6095e+01,  1.6256e+01, -3.9061e+00, -1.9038e+01,  1.5503e+01],\n","        [ 2.4451e-01, -1.8833e+00, -2.2911e+00, -1.1689e+00, -1.4611e+00,\n","         -2.2158e+00,  2.5781e+00, -3.7414e+00, -3.6355e+00,  3.0253e+00],\n","        [-2.9072e+00, -1.1402e+01, -4.8928e+00, -3.1647e+00, -1.8920e+00,\n","         -1.0039e+01,  1.1668e+01, -7.2033e+00, -1.3642e+01,  6.0406e+00],\n","        [-1.9898e+00, -7.3946e+00, -3.9262e+00, -2.1359e+00, -1.3204e+00,\n","         -5.4595e+00,  3.5389e+00, -8.4539e+00, -9.9485e+00,  7.7000e+00],\n","        [-4.1679e+00, -1.4126e+01, -3.4472e+00, -2.8376e-01, -2.5486e+00,\n","         -8.8043e+00,  9.4622e+00, -2.8916e+00, -1.2608e+01,  8.8332e+00],\n","        [-1.9438e+00, -2.5843e+00, -1.1344e+01,  9.6321e-01, -5.3549e+00,\n","          2.2212e+00,  3.5895e+00, -9.8994e+00, -8.3443e+00,  1.0965e+01],\n","        [-4.8120e+00, -1.5489e+01, -8.7883e+00,  2.1843e+00, -6.6846e+00,\n","         -1.2160e+01,  1.0102e+01, -6.4180e+00, -1.5881e+01,  8.2178e+00],\n","        [-2.2567e+00, -6.5114e+00, -2.4569e+00, -5.0772e+00, -4.5583e+00,\n","         -9.3934e+00,  1.3844e+01, -4.6414e+00, -1.2961e+01,  5.8518e+00],\n","        [-5.7708e+00, -1.8843e+01, -5.3750e+00, -2.7169e+00, -4.6348e+00,\n","         -1.5159e+01,  1.6630e+01, -6.3660e+00, -2.1320e+01,  1.1064e+01],\n","        [-3.0184e+00, -2.8875e+01, -1.4439e+01, -4.3855e+00, -7.9652e+00,\n","         -1.3748e+01,  5.5448e+00, -1.4475e+01, -2.2545e+01,  1.2679e+01],\n","        [-1.8449e+00, -9.9271e+00, -4.2787e+00, -2.3269e+00, -1.0773e+00,\n","         -6.2462e+00,  1.1243e+00, -1.2549e+01, -1.1940e+01,  9.1322e+00],\n","        [-6.0709e+00, -2.6700e+01, -8.9354e+00,  3.8052e+00, -1.1956e+00,\n","         -1.0053e+01,  1.1106e+01, -3.7610e+00, -1.8562e+01,  1.8409e+01],\n","        [-4.2981e+00, -5.1187e+00, -2.7064e+00,  7.3669e-01, -4.1309e+00,\n","         -6.8708e+00,  6.6880e+00, -2.3228e+00, -7.9995e+00,  8.2766e+00],\n","        [-3.1302e+00, -7.4118e+00, -6.3251e+00,  3.0960e+00, -4.9866e+00,\n","         -7.1309e+00,  5.3057e+00,  1.3179e+00, -6.5990e+00,  1.0200e+01],\n","        [-4.6547e+00, -1.6415e+01, -9.1034e+00,  7.0182e+00, -4.7946e+00,\n","         -4.9259e+00,  5.0102e+00, -2.6477e+00, -1.3801e+01,  1.5157e+01],\n","        [-9.9700e-01, -1.5542e+01,  8.8335e-01, -9.4920e-01,  6.1944e-01,\n","         -5.3463e+00,  5.6527e+00,  4.6515e-01, -4.4602e+00,  6.5902e+00],\n","        [-3.5170e+00, -1.2972e+01, -1.1033e+01,  6.4945e+00,  1.0499e+01,\n","         -2.3241e-02,  1.1521e+00,  9.8366e-01, -1.4225e+01,  1.1931e+01],\n","        [-4.2456e+00, -5.7742e+00, -3.0496e+00,  4.8513e+00,  3.8474e+00,\n","         -3.6745e+00,  5.3917e-01, -2.0160e+00, -7.8019e+00,  9.6068e+00],\n","        [-1.0155e+00, -6.0668e+00, -3.5363e+00, -8.0746e-01, -2.1659e+00,\n","         -6.6814e+00,  5.6298e+00, -5.0167e+00, -7.8922e+00,  3.2996e+00],\n","        [-3.0977e+00, -1.9589e+01, -4.1489e+00, -1.9296e+00,  1.7521e+00,\n","         -8.8390e+00,  9.8333e+00, -5.4223e+00, -1.4248e+01,  6.1160e+00],\n","        [-1.7957e+00, -1.2301e+01, -4.2862e+00, -4.9544e+00, -1.9660e+00,\n","         -1.0371e+01,  1.2302e+01, -4.8581e+00, -1.4090e+01,  9.0764e+00],\n","        [-9.7795e+00, -2.1915e+01, -1.0668e+01,  6.3794e+00, -5.5090e+00,\n","         -1.1783e+01,  1.1702e+01, -1.9809e+00, -1.7136e+01,  1.8611e+01],\n","        [-2.6095e+00, -8.2453e+00, -2.4597e+00, -5.0117e+00, -6.1542e+00,\n","         -1.2578e+01,  1.7554e+01, -2.9929e+00, -1.3809e+01,  1.1516e+01],\n","        [-3.6636e+00, -3.8724e+00, -5.1276e+00, -1.9982e+00, -4.1609e+00,\n","         -4.7112e+00,  5.4746e+00, -3.5984e+00, -8.3952e+00,  9.7544e+00],\n","        [-1.3808e+01, -1.0463e+01, -4.5441e+00,  1.0898e+01,  4.7910e-01,\n","         -1.9230e+01,  9.6235e+00,  6.9655e-01, -1.4833e+01,  2.6167e+01],\n","        [-3.0647e+00, -1.7849e+01, -2.5729e+00,  6.0272e-02, -8.6903e-01,\n","         -9.8243e+00,  9.2520e+00, -2.0311e+00, -1.1176e+01,  8.6344e+00],\n","        [-2.4640e+00, -1.1943e+01, -7.0090e+00,  1.7968e+00,  3.3673e+00,\n","         -4.0781e+00,  3.5368e+00, -1.6015e+00, -8.3194e+00,  7.1841e+00],\n","        [-2.6429e+00, -6.6173e+00, -2.3416e+00,  2.7473e+00,  5.4476e+00,\n","         -2.3107e+00, -2.1440e+00, -4.9390e+00, -8.0002e+00,  7.5246e+00],\n","        [-2.1361e+00, -9.3490e+00, -1.4706e+00, -3.2177e+00, -9.4735e-01,\n","         -6.4283e+00,  9.0235e+00, -3.4248e+00, -9.7240e+00,  3.6126e+00],\n","        [ 9.3915e-01, -1.0038e+01, -1.9927e+00, -3.6653e+00,  1.3652e+00,\n","         -2.7593e+00,  2.9636e-01, -1.1166e+01, -8.9627e+00,  5.9813e+00],\n","        [-6.0653e-01, -3.6754e+00, -1.7250e+00, -3.6950e-01,  9.1701e-01,\n","         -4.5622e-01, -1.4636e+00, -4.8995e+00, -4.2354e+00,  3.3504e+00],\n","        [-7.2411e-01, -8.2072e+00, -4.4223e+00, -7.0332e-01,  2.1128e+00,\n","         -6.4023e-01,  2.6147e+00, -2.6114e+00, -6.5770e+00,  2.5559e+00],\n","        [-8.1239e+00, -9.5300e+00, -9.6559e+00,  1.1117e+01, -6.4903e-01,\n","         -7.0514e+00,  3.1316e+00, -2.3285e+00, -1.0168e+01,  1.3675e+01],\n","        [-9.8916e-01, -4.1768e+00, -8.2034e-01, -2.0226e+00, -2.3076e+00,\n","         -7.1675e+00,  7.7049e+00, -7.7308e-01, -5.1421e+00,  8.2029e+00],\n","        [-1.6987e+00, -1.3847e+01, -4.1445e+00, -6.5380e+00, -6.3686e-02,\n","         -7.5503e+00,  1.3090e+01, -9.3142e+00, -1.4160e+01,  3.7368e+00],\n","        [-4.6109e+00, -1.5662e+01, -3.6872e+00, -4.4939e+00, -3.4414e+00,\n","         -1.3965e+01,  1.6572e+01, -5.8120e+00, -1.9190e+01,  8.0534e+00],\n","        [-3.5087e+00, -1.1899e+01, -7.8351e+00,  4.6658e+00,  4.8830e-01,\n","         -1.9930e+00,  3.1511e+00, -1.1172e+00, -7.6861e+00,  1.1195e+01],\n","        [-2.8494e+00, -1.2611e+01, -3.8825e+00, -7.8366e+00, -3.4465e+00,\n","         -1.2549e+01,  1.8738e+01, -8.6394e+00, -1.8491e+01,  5.9480e+00],\n","        [-1.0177e+01, -9.0663e+00, -3.5431e+00,  1.0192e+01,  9.1932e+00,\n","         -1.1175e+01,  2.3143e+00, -2.3621e+00, -1.3324e+01,  1.8763e+01],\n","        [-4.7393e+00, -1.7903e+01, -7.0725e+00,  4.3402e+00, -1.7825e+00,\n","         -8.0207e+00,  7.5314e+00, -3.7704e+00, -1.4397e+01,  1.4012e+01],\n","        [ 6.9292e-01, -5.8998e+00, -4.0729e+00, -3.4477e+00, -2.8826e+00,\n","         -2.2646e+00, -6.9632e-01, -8.6843e+00, -6.7853e+00,  3.5151e+00],\n","        [-6.3594e+00, -1.5469e+01, -4.7239e+00,  1.0367e-02, -4.4491e+00,\n","         -1.0849e+01,  1.1651e+01, -3.9982e+00, -1.5747e+01,  1.2289e+01],\n","        [ 6.8216e-01, -1.0248e+01,  3.2988e-01, -2.0438e+00,  2.1514e+00,\n","         -1.5200e+01,  1.1385e+01,  1.0871e+00, -1.0847e+01,  9.6580e+00],\n","        [-2.9170e+00, -1.4039e+01, -5.7978e+00, -1.2013e+00, -9.0600e-02,\n","         -6.4476e+00,  5.9970e+00, -9.7174e+00, -1.3555e+01,  1.0370e+01],\n","        [ 1.5149e-02, -6.5359e+00, -3.7193e-03, -2.9578e+00, -1.4255e+00,\n","         -3.8479e+00,  6.1464e+00, -1.5647e+00, -5.0424e+00,  4.1078e+00],\n","        [-4.9562e+00, -2.3710e+01, -4.0385e+00, -2.5703e+00, -2.5620e+00,\n","         -1.5781e+01,  1.6433e+01, -4.7515e+00, -2.0515e+01,  1.0286e+01]],\n","       grad_fn=<AddmmBackward0>)\n","torch.Size([64, 10])\n","\n","ground thruth\n","tensor([[2],\n","        [9],\n","        [6],\n","        [0],\n","        [3],\n","        [4],\n","        [4],\n","        [5],\n","        [4],\n","        [8],\n","        [0],\n","        [8],\n","        [9],\n","        [0],\n","        [2],\n","        [2],\n","        [9],\n","        [3],\n","        [3],\n","        [3],\n","        [8],\n","        [7],\n","        [4],\n","        [4],\n","        [0],\n","        [4],\n","        [4],\n","        [8],\n","        [7],\n","        [1],\n","        [5],\n","        [0],\n","        [5],\n","        [3],\n","        [2],\n","        [7],\n","        [3],\n","        [4],\n","        [2],\n","        [1],\n","        [6],\n","        [0],\n","        [9],\n","        [6],\n","        [0],\n","        [5],\n","        [6],\n","        [7],\n","        [7],\n","        [2],\n","        [5],\n","        [2],\n","        [2],\n","        [4],\n","        [1],\n","        [4],\n","        [9],\n","        [8],\n","        [3],\n","        [4],\n","        [5],\n","        [5],\n","        [6],\n","        [3]])\n","torch.Size([64, 1])\n"]}]},{"cell_type":"markdown","source":["# 작성한 dataset과 model을 이용하여 딥러닝 프로세스 구현  \n","\n","pytorch 딥러닝 프로세스\n","1. dataset, model선언\n","2. dataset과 model을 통한 결과 예측\n","3. 예측된 결과를 통해 **loss**연산 및 **loss.backward**\n","4. **optimizer.step()**를 사용하여 graident update\n","\n","주요 오브젝트  \n","torch.nn.CrossEntropyLoss: classification를 위한 loss 수행  \n","torch.optim.Adam: Adam optimizer를 통해 gradient update를 수행하는 class\n","\n","각 오브젝트의 입력과 선언은 다음 doc 참조:  \n","CrossEntropyLoss: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html  \n","Adam: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html"],"metadata":{"id":"O6yaLlOZMFOx"}},{"cell_type":"code","source":["from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","\n","#학습을 위한 optimizer와 loss function 설정\n","lf = CrossEntropyLoss()\n","optimizer = Adam(model.parameters(), lr=0.001)\n","\n","#100번의 epoch을 실행\n","for e in range(100):\n","  print(\"\\nepoch \", e)\n","  epoch_loss = 0\n","\n","  #선언한 model을 학습 가능한 상태로 변경\n","  model.train()\n","\n","  #모든 학습데이터에 대해서 학습\n","  for i in trainDataloader:\n","    #매 배치에 대한 gradient계산 이전에 optimizer에 저장된 이전 batch에 gradient를 삭제(초기화)\n","    optimizer.zero_grad()\n","\n","    data = i[0]\n","    target = i[1]\n","\n","    #결과 도출\n","    output = model(data)\n","\n","    #loss연산\n","    loss =lf(output, target.squeeze())\n","\n","    #loss backpropagation\n","    loss.backward()\n","\n","    #gradient update\n","    optimizer.step()\n","    epoch_loss += loss.item()\n","\n","  print(\"train loss\", epoch_loss/len(trainDataloader))\n","\n","  #model이 학습되지 않는 상태로 변경\n","  model.eval()\n","  test_loss = 0\n","\n","  #gradient를 계산하지 않도록 하여 cost낭비 방지\n","  with torch.no_grad():\n","    #모든 test dataset에 대해서 결과연산\n","    for i in testDataloader:\n","      data = i[0]\n","      target = i[1]\n","\n","      output = model(data)\n","\n","      loss = lf(output, target.squeeze())\n","      test_loss += loss.item()\n","\n","      pred_label = torch.argmax(output, dim=-1)\n","      # print(sum(pred_label == target.squeeze))\n","\n","  print(\"test loss\", test_loss/len(testDataloader))\n","\n"],"metadata":{"id":"NNW-Nuz2M3GY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692258299086,"user_tz":-540,"elapsed":271860,"user":{"displayName":"Wooseok Kim (Wooseok)","userId":"10829587564796253131"}},"outputId":"9b112195-b863-4971-c5ae-e24ad16dae22"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","epoch  0\n","train loss 1.7072110085547725\n","test loss 1.658403143286705\n","\n","epoch  1\n","train loss 1.6092683303205273\n","test loss 1.6677372753620148\n","\n","epoch  2\n","train loss 1.6100349260281912\n","test loss 1.6214602813124657\n","\n","epoch  3\n","train loss 1.6019917168194735\n","test loss 1.7606471255421638\n","\n","epoch  4\n","train loss 1.6038827956477297\n","test loss 1.5996231734752655\n","\n","epoch  5\n","train loss 1.5984083987489532\n","test loss 1.59738277643919\n","\n","epoch  6\n","train loss 1.6144927468480943\n","test loss 1.6437721848487854\n","\n","epoch  7\n","train loss 1.6219052181968205\n","test loss 1.6222265809774399\n","\n","epoch  8\n","train loss 1.590532340580904\n","test loss 1.6892364993691444\n","\n","epoch  9\n","train loss 1.6138902464999427\n","test loss 1.8889990001916885\n","\n","epoch  10\n","train loss 1.620734522614298\n","test loss 1.8029621690511703\n","\n","epoch  11\n","train loss 1.6610238491734373\n","test loss 1.6321982890367508\n","\n","epoch  12\n","train loss 1.6906937330583982\n","test loss 1.7462877109646797\n","\n","epoch  13\n","train loss 1.9429145025301584\n","test loss 1.8707768842577934\n","\n","epoch  14\n","train loss 1.6512424628945845\n","test loss 1.8492879197001457\n","\n","epoch  15\n","train loss 1.6364442291139047\n","test loss 1.7938033863902092\n","\n","epoch  16\n","train loss 1.6057550137556051\n","test loss 1.7353625744581223\n","\n","epoch  17\n","train loss 1.5982442596290685\n","test loss 1.663801185786724\n","\n","epoch  18\n","train loss 1.5959167163583297\n","test loss 1.7125366926193237\n","\n","epoch  19\n","train loss 1.6210175420664534\n","test loss 1.8872082233428955\n","\n","epoch  20\n","train loss 1.6076374385930314\n","test loss 1.9094779789447784\n","\n","epoch  21\n","train loss 1.6055149413362335\n","test loss 1.6603631377220154\n","\n","epoch  22\n","train loss 1.6216943309276919\n","test loss 1.7947154268622398\n","\n","epoch  23\n","train loss 1.594757179670696\n","test loss 1.7138868942856789\n","\n","epoch  24\n","train loss 1.6116354178778733\n","test loss 1.6217581033706665\n","\n","epoch  25\n","train loss 1.620171792899506\n","test loss 1.6741219535470009\n","\n","epoch  26\n","train loss 1.6013509065290041\n","test loss 1.815135046839714\n","\n","epoch  27\n","train loss 1.5880271332173408\n","test loss 1.6715309619903564\n","\n","epoch  28\n","train loss 1.5819608923755115\n","test loss 1.644667610526085\n","\n","epoch  29\n","train loss 1.598919741715057\n","test loss 1.6562576070427895\n","\n","epoch  30\n","train loss 1.5966542732866504\n","test loss 1.582764282822609\n","\n","epoch  31\n","train loss 1.6644172306302227\n","test loss 1.7870600372552872\n","\n","epoch  32\n","train loss 1.6425055054169666\n","test loss 1.5882707461714745\n","\n","epoch  33\n","train loss 1.6008463313307943\n","test loss 1.5823982879519463\n","\n","epoch  34\n","train loss 1.5813379891311066\n","test loss 1.5847923383116722\n","\n","epoch  35\n","train loss 1.6091642289222041\n","test loss 1.5854720771312714\n","\n","epoch  36\n","train loss 1.5921519542042213\n","test loss 1.573775514960289\n","\n","epoch  37\n","train loss 1.5899498387228084\n","test loss 1.5984078869223595\n","\n","epoch  38\n","train loss 1.5743644720391383\n","test loss 1.76457779109478\n","\n","epoch  39\n","train loss 1.5896934482115734\n","test loss 1.6057687178254128\n","\n","epoch  40\n","train loss 1.578803080546705\n","test loss 1.6122058629989624\n","\n","epoch  41\n","train loss 1.5878048199641555\n","test loss 1.6630424708127975\n","\n","epoch  42\n","train loss 1.5900503985489471\n","test loss 1.7083643600344658\n","\n","epoch  43\n","train loss 1.5767470779298227\n","test loss 1.756067119538784\n","\n","epoch  44\n","train loss 1.6081600491004655\n","test loss 1.6695942282676697\n","\n","epoch  45\n","train loss 1.5952988772452632\n","test loss 1.6312492862343788\n","\n","epoch  46\n","train loss 1.5831407987618749\n","test loss 1.6080882549285889\n","\n","epoch  47\n","train loss 1.573920103568065\n","test loss 1.5943304300308228\n","\n","epoch  48\n","train loss 1.6205952786192108\n","test loss 1.6254686117172241\n","\n","epoch  49\n","train loss 1.585623804526993\n","test loss 1.6266937851905823\n","\n","epoch  50\n","train loss 1.565882937817634\n","test loss 1.6124115958809853\n","\n","epoch  51\n","train loss 1.615557727934439\n","test loss 1.7026722803711891\n","\n","epoch  52\n","train loss 1.5914984820764275\n","test loss 1.5911232829093933\n","\n","epoch  53\n","train loss 1.5727864684937876\n","test loss 1.6057164072990417\n","\n","epoch  54\n","train loss 1.5978964567184448\n","test loss 1.640768475830555\n","\n","epoch  55\n","train loss 1.564456262165987\n","test loss 1.5763895735144615\n","\n","epoch  56\n","train loss 1.6783713705932037\n","test loss 1.772622711956501\n","\n","epoch  57\n","train loss 1.6493124961853027\n","test loss 1.6395471766591072\n","\n","epoch  58\n","train loss 1.6170484732977952\n","test loss 1.6363680511713028\n","\n","epoch  59\n","train loss 1.5783601124075395\n","test loss 1.6030055955052376\n","\n","epoch  60\n","train loss 1.5771946167644066\n","test loss 1.6019649803638458\n","\n","epoch  61\n","train loss 1.566539643686029\n","test loss 1.612533025443554\n","\n","epoch  62\n","train loss 1.557192459891114\n","test loss 1.58695887029171\n","\n","epoch  63\n","train loss 1.571251107167594\n","test loss 1.6300579011440277\n","\n","epoch  64\n","train loss 1.5654681257054777\n","test loss 1.619739145040512\n","\n","epoch  65\n","train loss 1.559163666978667\n","test loss 1.5893730521202087\n","\n","epoch  66\n","train loss 1.6158569930474969\n","test loss 1.7008413523435593\n","\n","epoch  67\n","train loss 1.6099595555776283\n","test loss 1.6465024799108505\n","\n","epoch  68\n","train loss 1.5909404120867765\n","test loss 1.8428132757544518\n","\n","epoch  69\n","train loss 1.6009266467034062\n","test loss 1.7938649579882622\n","\n","epoch  70\n","train loss 1.569945929925653\n","test loss 1.6532155871391296\n","\n","epoch  71\n","train loss 1.8555179291133639\n","test loss 2.199580132961273\n","\n","epoch  72\n","train loss 1.6791416029386883\n","test loss 2.042803533375263\n","\n","epoch  73\n","train loss 1.6477084431467177\n","test loss 1.9391025304794312\n","\n","epoch  74\n","train loss 1.6130534380297117\n","test loss 1.6214989498257637\n","\n","epoch  75\n","train loss 1.5881638225120833\n","test loss 1.6080977767705917\n","\n","epoch  76\n","train loss 1.568382982966266\n","test loss 1.7080282717943192\n","\n","epoch  77\n","train loss 1.6015241146087646\n","test loss 1.6935176476836205\n","\n","epoch  78\n","train loss 1.563390556770035\n","test loss 1.6076536998152733\n","\n","epoch  79\n","train loss 1.5781204549572136\n","test loss 1.6021906659007072\n","\n","epoch  80\n","train loss 1.5538774728775024\n","test loss 1.6166638359427452\n","\n","epoch  81\n","train loss 1.5433852732936038\n","test loss 1.611745610833168\n","\n","epoch  82\n","train loss 1.580387895620322\n","test loss 1.635854221880436\n","\n","epoch  83\n","train loss 1.565436601638794\n","test loss 1.6116128265857697\n","\n","epoch  84\n","train loss 1.5639961716494983\n","test loss 1.6121030300855637\n","\n","epoch  85\n","train loss 1.621216279041918\n","test loss 1.6350606456398964\n","\n","epoch  86\n","train loss 1.6453132055982758\n","test loss 1.6213741675019264\n","\n","epoch  87\n","train loss 1.573099222364305\n","test loss 1.6442713737487793\n","\n","epoch  88\n","train loss 1.5811108562010754\n","test loss 1.6328033059835434\n","\n","epoch  89\n","train loss 1.550177791450597\n","test loss 1.622155249118805\n","\n","epoch  90\n","train loss 1.5406316126449198\n","test loss 1.627992369234562\n","\n","epoch  91\n","train loss 1.5693556402302995\n","test loss 1.6137490198016167\n","\n","epoch  92\n","train loss 1.5414060671118242\n","test loss 1.6328190490603447\n","\n","epoch  93\n","train loss 1.5401067884662483\n","test loss 1.7590145468711853\n","\n","epoch  94\n","train loss 1.5462942108323303\n","test loss 1.6120819374918938\n","\n","epoch  95\n","train loss 1.543560400793824\n","test loss 1.6445941925048828\n","\n","epoch  96\n","train loss 1.5399267386786546\n","test loss 1.6437208503484726\n","\n","epoch  97\n","train loss 1.5486775878109509\n","test loss 1.6675984859466553\n","\n","epoch  98\n","train loss 1.564471038081978\n","test loss 1.653656728565693\n","\n","epoch  99\n","train loss 1.5354589012604725\n","test loss 1.6374876648187637\n"]}]}]}